So the news from me are that, we had session like live workshop dry run at the [LOCATION1] on Wednesday and all the presentations went kind of well for the people who could understand [OTHER4], because that was run in [OTHER4], and people who had to rely on our subtitling were totally lost.
So, the, the head of the particle division of the [ORGANIZATION4] was at first very angry like not, not allowed to, to everybody, but I was talking to him for more than five minutes, maybe ten minutes in the corridor.
And he was expecting that the technology will be already useful for the users and I've confirmed that the technology is now only working technically.
So we, we knew this beforehand, so the next the planned session with [PERSON7] that we have is, we don't have date for that yet.
And we would like, so it has to be something that we can ASR, and [OTHER2] is the, the best next option, and we will try to rely on the live generated translations into [OTHER4], for example, and we will see how bad the translations will be.
And we will also diagnose, the flicker, because for, if you don't understand the source language, then you probably prefer longer delay, and less of flicker, but that's something that has to be tested.
So that's, that's one piece of news another bit is that the critically required, and as part of the usability for the users, the critical required the video and subtitles or the slides, and subtitles to be on the same screen.
And if the screenshots change, um, then it will, it will send it as an updated picture to the, to the browser, so the client will be, simply displaying the most recent picture.
(PERSON8) Yes, but it's, I don't think it's too bad solution, because it's intended for slides, there is usually no, it's, it's not good for videos obviously, there is no way this would work for videos, but it's good for static slides, and the setup that we have on Monday seminars all the time, and, um.
And the, on the, on the Monday seminars the static slides they even, they even hide the, the watch, so there is like clock.
So that's, that's, that's the current thing that we have, but we still need to somehow organize the the layout to provide subtitles next to either subtitles or the paragraph you next to the slides.
So, [PERSON2] agreed to join us for the surge, depending on his availability he's or his some some paper to finish that, like project to finish, to make into paper, and then also help us in whatever is needed, so that the the surge documents.
I'll be again inviting people to help, and I was curious, especially about [PERSON6], if he was like successful with his exams, and if he could now work on domain adaptation, so that we could feed in computational linguistics language model for, like [OTHER4] talk on computational linguistics, and if that would be, if we if we could plug this into the into the current setup.
Again, I'm reminding you of, of the sheet, please enter your achievements, so as soon as you've accomplished something, add to the line there, otherwise we'll forget who did what, and that's too bad for the bonuses.
I, I've sent an email to the <unintelligible>, about the, what I found, so if anyone could look into it, and say something, it would be great.
Yeah, okay, so we'll <unintelligible>, yeah, it's it's something that goes beyond the the time limit that that we have now.
So the idea is that we need to provide all the forms, so as soon as a word is found in, because your processing all the related materials, if the related materials, if we're focusing on [LOCATION2], if the related materials contain one word.
And, I dunno know, what is the the the key part of of your question, I would for simplicity assign all the words all these forms of the same word, the same probability.
And so that's feminine known "reference", then I would use that list of all [LOCATION2] word forms to generate all the forms of this word, so "reference, "referenci", "referenci" this is actually boring, so this, this is very few.
The technical process, I should process the, the dictionary, because you mentioned some, you mentioned the <unintelligible> transcript for using that, but from my findings, I'm not sure if it that's necessary.
So, is there any chance that you would have some files ready for the Monday talk, or not at all if not that's not a problem, either, but there is all of the semesters are starting, so every Monday there will be some more technical talk in [OTHER4], mostly in [OTHER4] on <unintelligible>
(PERSON4) Okay, so I have <unintelligible> mostly the work and the <unintelligible>, I have it for [LOCATION2], but <unintelligible> too difficult to modify for [OTHER4], so, um, I <unintelligible>, if I received some domain input sentences, then I could search the database, and return a list of a singular sentence states, and some large focus, and it should also work on word level, so if I give it uh, <unintelligible>, then it will find a sentences that are like <unintelligible>, so it maybe, because I will be in the mountains on the weekend, but I will be back in like Sunday night, so <unintelligible> for the Monday, and then I will be able to provide something, but probably either like Monday morning or Sunday late night, <unintelligible>.
Thank you, so I don't think that <unintelligible> ready and used for this Monday, but it will be very good if you synchronize with [PERSON6], and if you provided this to [PERSON6]
[PERSON6] would use the files that we like find for the speaker, and as soon as we have the the the either you or [PERSON6], well that doesn't matter who does that.
The back end is a server in C++, which is just reads the has gently reads the microphone signal, and then it sends it over web socket to the client, which is just a JavaScript piece of code that can be included in any webpage even on a remote machine.
So that could make it possible to visualize the sound from the microphone the longer we, we have the volume.
I also made it so that it colors in different colors, depending on the volume, though, the the last problem is that I'm not sure how to stand those thresholds, and I wrote into email to [PERSON7] that that I could do in in like three ways either I could just like.
Or I could just choose a recording from the from the minuting corpus, and I could perhaps, play the recording for myself and see how loud it is, and then see whether the.
the level, and and as indicated or is calculated by your application, and then sat the threshold, so that they work for this, and if you are running any problem problems just set it to some random value.
It's only for indication so far, so we need to to like to polish the the the indicators, and it's not, it's not critical in this development stage.
So maybe if the output could be a little bit more verbose so, that some number, would appear next to the next to the the image as well, so that when [PERSON7] is observing the session, and he notices that something is too quiet, and or too loud, he could mark down the number, so that he doesn't have to take a screenshot and then.
(PERSON9) The problem is is that, the problem is just that, because it was trained with TPU that, there are some other like, ways of how it, how it saves the models.
And I also received new data from [ORGANIZATION2], and I'm going to get it training today, and hopefully I will have some results, until until the end of the weekend.
(PERSON9) But I had to do it just in <unintelligible>, because of the TPU support, so so most likely I will have to export it as.
So we we don't have uh, the the connection to the the integration of [OTHER3] models, do we?
(PERSON8) Yeah, and see if we, so that's a thing for [PERSON7] again, as we are growing, we still have not collect, obtained any single number from our models, in the in as they are in this setup, but we should get to that in in in the two weeks of February at the latest, so within these two weeks.
(PERSON9) But it is, but it is trained like in [OTHER4] centric way, and and when I evaluated it on [OTHER4], it has much better performance than in the other languages obviously, because it sees [OTHER4] all the time.
(PERSON8) Yeah, that's the paraphrasing, but for [PROJECT2] purposes, simply the send all the various recordings that we have already transcribed, and we don't have translation for that, so we need to polish the data set, but just for.
So the best corpus is actually the the best file in the <unintelligible> file which you have also translated into [OTHER5].
So please [PERSON1] send the path, [PERSON5] has access to the to the clusters and [PERSON5] can run all the competition.
So please send the path to all the source files that you have to [PERSON5] and [PERSON5] will apply the models, and we will get the first follow synthetic, very much in domain corpus, across these six main languages, and then redo this also with all the other 30 something languages.
(PERSON4) Just let you know, I'm just going back next week, I work part time, so that's the reason I just had limited time for this project, and most of my tasks are <unintelligible> translations from [OTHER4] to [OTHER5] to feed anything <unintelligible>.
(PERSON7) Hi everyone, so I'll just make it quick <unintelligible> how much this week, so I spend most of the time like preparing for the dry run and [ORGANIZATION4] event which was yesterday.
The, and I'm not able to run the script basically to deploy their worker on the mediator, and the installation was succesful, so this is like few our tasks, which I concerned <unintelligible>.
(PERSON8) And another question, for what you are training on, because if you are training on correct text, and that's the case, then there is like a domain mismatch of looked again, and a few sentences that [PERSON11] was like saying in his [LOCATION2] presentation.
And naturally, he is a repeating phrases it's jumping out of the sentence, and then and then coming back to the sentence and this is some, this is a second output which we cannot handle.
The the the fact that the [PERSON4] sentences are on the same topic or whether the sentences are as the influent as in normal speech.
(PERSON9) I have a quick idea, could we perhaps make just something like, Wikipedia page, or something like that with at least all possible data sets or all possible like you know purposes, so that so that when someone like has a new one, we can always just add it there, so it can be used by everyone else.
(PERSON9) Yeah, okay, okay, because like I, I also have some processed data that I collected, and so, so I could send it somewhere and so on.
So, thank you very much for coming, next I'll email everybody again, at the latest on Wednesday, when are we meeting next week, and there is Doodle <unintelligible>, so please keep that Doodle <unintelligible>, like up to date because the Doodle <unintelligible> applies to regular weekly time, and we will find the date, which time which suits most of you.
