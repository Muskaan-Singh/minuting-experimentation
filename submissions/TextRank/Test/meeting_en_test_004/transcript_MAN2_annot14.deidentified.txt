One of the command is running only the ASR worker and the other command the lower window is running both, the ASR worker and the machine translation into Czech.
So the German signal will be available only like for 20 minutes, and then 20 minute break and 20 minutes again, because it will be only one person attending the the last 20 minutes, and they they have to take break.
But anyway, we should be ready for for that, because the German from the ASR will be much better and German speakers will be much happier to see uh, the the human interpretation, but we should be testing everything.
So I will try to get the batteries, ehm, and you're right that- the good thing about German ASR is that we don't rely on the German, on the English segmentation worker so that that could run as well.
You can see me and [PERSON1] is also holding the second mike, so that you hear what I 'm saying, and you are seeing the translations with the delay one, two three, so yes,
(PERSON4) The segmentation is <unintelligible> getting the end of one sentence in the beginning of the next, and we not read it preparefull that, and I think that's something that we need to talk to [ORGANIZATION3] they can fix that and make sure that they're not getting these overlapped to two sentences but that's a tranlastions starts.
And you have to click this view full motion presentation so that the frame rave of Pexip is- is higher and that you will see what exactly is rolling on the ehm on the clients of the ASR.
So, the, the list of icons that I was- So the the issue that we are trying to to help you solve now is that when you are following the the screen presented by [PERSON1], [PERSON1] are you still presenting the screen?
(PERSON7) It could- It could work also asking, for example, for multiple eh translations that's giving the input audio file, and they want, for example the germ-German audio file and that want in to receive the English translation and the French translation.
(PERSON7) No, no, it should be the same issue, but I also, I'm a little bit confused the on this topic, than probably I haven't explain it very well.
So the the alti ee match best solution is that the ASR worker double checks it's output and never sends the same string ehm-If-Well I should very different list still.
And and [PERSON1]'s notebook not recording anything yet, but the beringer we have, the sound in the sound card, but we do not have the sound in the system, and it's stopped by itself, or have you been changing any of the sound settings?
(PERSON7) There is the - this is a demo of course but when we have the M3U8 URL, we can configure the presentation platform to connect to this streaming and display the provide thing.
Can we get access to that, or maybe if you do not want to share the whole thing, then just the video client, so that we can test the video client.
At least the webpage that shows the video and this is the thing that we need to link with the screencast that we are going to to send.
(PERSON9) So If the idea is that to simply send to start the client so that the target well signature is this eh, an EU pub, right?
(PERSON7) Yes, exactly.Actually, the the <unintelligible> worker is completely transparent, the packets he receives send back exactly as he has received them, but as a kind of processing it publish subtitle on the <unintelligible>
But also with characters in coding and we are debugging where the problems is originated, there are strange characters coming out from the ASR or if it's the Java integration whose not correctly managing the <unintelligible>.
(PERSON4) <unintelligible> in the document is that I said I'm not quite understand the logic in the ASR, because it's giving us sentences in <unintelligible> fragments <unintelligible>
We don't have access to the um, maybe you can, yes, if you know, you can tell us how to use the streaming from VLC that work for you.
(PERSON12) All right, the M3U8 format it's it's ehm list all the currently available chunks and the browser goes, again and again, fetching the M3U8 file, right?
I think we should we should try to wrap up, so I would like to ask everybody who is still on the call, to go through the the current state of the Google document, and add whatever we have not recorded in the-
(PERSON7) Ok, if at the moment the definition of sources is static, it's not dynamic, it's pretty difficult to define exactly the logic of joining services and living services we're still reasoning about it.
So it's it's now like poppin- pokin- poppin- popping up one by one, we we've learn that if we add dash pub it will be grabbed by the worker for the presentation platform.
(PERSON9) We need the map because it's different institutions who are starting the different workers and if they misregistered the worker if they use like mismatching fingerprint there is no way to make use of that worker.
(PERSON9) No, no, it's like once we have the idea of what is being connected to what then we know what the inputs and outputs are and it's easy to use it.
(PERSON7) And you run the for example the EB client, I know it's pretty much an old tool, but when you run it one of the first thing it displays is exactly the path selected.
But what I'm saying is that if we don't have the map, and if someone from runs a worker with some incompatible fingerprint so that the worker from [ORGANIZATION5] does not connect to it, then the path would not be accesible.
(PERSON9) Yeah, so the point is that if there is a worker connected to the mediator, there is no way for us to change it's fingerprints.
(PERSON7) Actually that for the one who runs the client is is not available this information, and simply provides if <unintelligible> so there is no logic implemented in order to ehm change fingerprints to worker, because worker who be there or not or <unintelligible> will not be available or it's all delegated to who provides the service.
And do not edit the existing ones, so [PERSON7] could you please put all the fingerprints of the languages that the presentation platform responds you next to he object that I just created?
(PERSON7) It's not acting to the worker, it's acting only on the webpage, and it saying the language available for the, for the user.
So what is necessary in order to have the the languages as fingerprints the client can connect to and does give them the text.
(PERSON9) It is, it is, when [PERSON1], when [PERSON1] launches the client, I'll say the way I understand it, but you need to correct me if I'm wrong.
So I think that [PERSON1] launches EB client and tells EB client, here is some audio with this fingerprint it German audio, and I want the target fingerprint find me a path for that and my desired target fingerprint is Italian, Italian pub text.
And the the EB client connects to the mediator and the mediator find the path and the path will include the worker which digest Italian text and produces Italian pub.
So when I'm changing the set of languages and I'll add also the Spanish here, ES, this will affect obviously the buttons available on the front page.
And for this pipeline to to be access if all the path to be accessible there has to be the worker registered already with it mediator that emits the Spanish Spain pub fingerprint that I'll put this fingerprint.
(PERSON9) Yeah, so this is this is the full screen mode of the web browser, what you're seeing in the webcam from my Pexip, right?
So, so can you make sure it's for the workshop in two weeks now, it's more important to have it optimized for the desktop rather than for the mobile phone.
So, I think it's usable in the in the horizontal mode because then there is more, it's it's hard to say where is more space.
(PERSON12) And it seems it works for the cellphone very well that it adapts to the size of the screen, I wonder why does the same code cannot be used on the desktop.
Yesterday we actually we tried for the full screen mode to prepare let's say a kind of overlapping, you know the the <unintelligible>, but we haven't found a solution and it's actually it's it's nothing in problem to, to develop it.
(PERSON9) Yeah, and when I'm thinking of is, can you, could you, could you share with us the code for the not all, maybe maybe not all of the presentation platform but like the bare bones which has the text thing, like the logic that converse the the words to where is that being displayed?
(PERSON7) I'm reasoning about it because at the moment, the the worker, the publi- the publication worker returns exactly the packet recieved and not the one published.
But maybe it could send back the published subtitles so that you can catch the subtitle directly from the the client running.
(PERSON9) Or maybe, maybe it's if it's easy I don't know how easy is it in the code, but if you could register the worker when you're starting the worker that feeds the presentation platform.
So that the same worker would offer two output, one would be so that you can connect it and it doesn't change the the stream and the other one would be the one that we could then directly use as the console client.
And we need everybody, so I'll ask everyone on the call today at 1 to contribute to this map and everybody needs to check that their workers are like signing themself with the right, with the right fingerprints.
And try to clean up the Google document this one, on the prepare to the calls if I misunderstood something or if if there's if some of the notes don't make sense please make it explicit so that we,yeah.
