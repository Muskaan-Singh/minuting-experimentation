Okay, but, that you can use the [ORGANIZATION2] and you can use SL the the word the read the context word and manning.
What they do attention and and compution scores would interesting in the sense that in their <unintelligible> 60 internet use emm try to use sentences and test on harder, weirder like constructions to see introduction like emm for like still means that would <unintelligible> some basic characteristic.
So what the paper does is that they have distinct where they take attention between subject and verb, subjects and verbs and subjects and verb in in sentences.
[PERSON3] And it seems like the the attention actually still manages to find still manages to find the proper subjects for the nouns particular the <unintelligible> about higher layers.
Now it through doing [OTHER9]s but kind of stuff might also be <unintelligible> because they use like it's a confusion score the PSI too.
Okay so, okay so, you you can start with I don't know, if you can start with [PROJECT2].
You can, for example, you can try to probe for the for the for means, which are really like more semantic dependence relations like ac- for example actor.
Which is real actor and this was not as subject what is real actor that mean that in passive sentences it's it's the object.
[PERSON3] Is it a one one layer one layer what we need, it's basicly like more than one <unintelligible> .
Because we want to come to [LOCATION2] for I don't two weeks and collaborate on something with us.
So it will be managed to to you to know what we need will do here and so that you can contribute or I don't know.
But we haven't so far we haven't organized this call so we've do first create organized call here with the leader here of this group.
[PERSON5] Yeah and I checked there is a measure, which measure is (out) the attention matrice is a line with particulations and actually I got this from from other paper.
And and it's ee chunked down from much attention eh is focused of the first of words data in ee syntactic laytion.
And We've to look what are the patterns of the eeh what difference actually our results from (though) measure <unintelligible> for [PERSON2] proposed one from different photo eh like one observation was that (for the), let's say easier <unintelligible> find the determiners or multifiers.
[PERSON7] On the on the zero on the first layer the there are the attention looks on the previous on the next token so that's that's the determiners on the other.
But then it was quite good but say like the subject finding sub- object there's few one like one head that was specialized eeh looking for that particulation, let's say.
Ehm and it's also one head that's very very surprised look that (penny sour) for (relations) done for determiner is the multifiers it's subjects or objects and looks in the last layer.
Then look up what is the layers how was attention matrices is that looks like for for other sentences.
I think in mostly, yeah, it's said like for the first layer is like many just low close positional in some cases which some variations like <unintelligible> very focused <unintelligible> in the principe like bounce.
And for those the determiners modifiers those heads look like justs looking up the neighborhood but context the words just shift that one of it's one to way to find determiners and something <unintelligible> some clients is in that eeh variations that's look plus one, minus one, just with any fix pattern.
And the the higher layer  for components depend subjects was what was it was (balustrade) layer which have this rows was looking in to different eh eh chunks.
What's this items and like I think it's I think it's the the relations eeh let's say that can be drive from attention matrice some of correlate with (syntactic tree).
But it's shouldn't be any head look an cannot find, by any specific corelation, what we are looking, so it's it's says like -
[PERSON5] It's constructure which some heads correlates that some heads has connection that's correlate with relation in different laytions in syntactic tree but it also had like this different types relations adverb.
[PERSON7] Have have you visualised, sorry, have you visualise which which phrases or which which (balustrades) are are there because it seems that there are almost all possible and these if you take the shorter phrases or shorter (balustrades) there are also almost all all possible balustrades somewhere, or at least according my observations.
[PERSON5] You mean like the shorter phrases can be in like (balustrades) but like different (balustrades) in different heads?
Yeah, it seems they do different stuffs so it makes sense to try to seek which head those what try to hide them supportly, I guess.
I should say one thing we tried to have have the similar results here in a use not nevermind then use open NMT system and try to generate somethink like balustrades and we have many problems many problems with generating something similar, like similar to balustrades.
So, so, I I I want to say that it's not easy to eeh that there maybe different different settings and little bit difference settings of transformer that generates completely different results.
Very <unintelligible> from the balustrades the matrices, the attention mattrices are very (blurt) and there were no (balustrades) straight and we have to it was like, the option.
So I only want to say that there are many, many things that made many, many parametres that may chains and the chains and there is completely different, completely different attention it's an attention ways so, so, yeah.
The other papers don't don't report that there are some like balustrades maybe they have different different parametres a little bit, yeah.
[PERSON5] Actually, I wanted to try ehm extract the graph structures compare how how was like similar syntactic trees or or different types of trees we can (extract) from syntax.
[PERSON10] There is, yeah, there is the papers where they took syntactic trees so from from syntactic parser and use that to form the the input of machine translation systems so you would just use and put using graph (conclusion).
So the idea was that we could try to replicate this and to compare it with <unintelligible> the extract from the attentions.
[PERSON7] Yeah, okay, and we should also, [PERSON3], [PERSON3] send me the paper where they they they had two two heads which are trained differently.
With the objective function to to to be as similar as possible to the to the (dependency) trees if I if I (redisconnect) it.
So they they have two two synt- two heads or eeh that were trained to to be similar to a that attentions in these heads, attention to the syntactic <unintelligible>, syntactic (precedence) or syntactic -.
And by this training they also, they also report better better results improvement in they in they target these two heads like gold syntax.
And they, yeah, they train it together so they don't need they don't need the syntax annotation when translating so it's it's learnt.
[PERSON3], [PERSON3], I will I will send you the links to the tactogrammatic when you can when you can download the treebank and the other link you need and other other anything you need to start experimenting, and, yeah.
So, if you have anything else, yeah, we can quit this call and and see you in next 14 days and I will be in contact with [PERSON3], too.
Okay, so, so what what you that there are you working on, [PERSON3], cause it seems you two are basicly working on similar stuff  -
[PERSON10] Cause cause I know you read the paper as well I don't think need to talk about this one.
So, so, you and I and someone else can be language model so [PROJECT2] language model or you can be a model payed person possibly task and then he could think okay, so, for doing machine translation it be into syntax or maybe it's useless the syntax.
Right, yeah, what I thinking it's okay, should be really continue analysing the transformers that we have or should we <unintelligible> someone <unintelligible>.
As are assumption okay, something like syntax <unintelligible> we have it's layers it's to have layers model analyse small layers so easier to find <unintelligible> that <unintelligible>.
[PERSON6] I think he was planning to come back like before I will supposed to I was supposed to be 8th of December.
And I think that he was planning to stay here after that but then <unintelligible> deserving <unintelligible> like maybe going back for another 2 weeks after New year's.
